from datasets import load_dataset
from text.method.dataset.clean_the_dataset import DatasetCleaner
from text.method.dataset.jaccard_deduplication import clear_global_memory, quick_jaccard_deduplicate
import time
import os
from tqdm import tqdm

def chunked_deduplication(dataset, text_field, threshold, ngram_size, chunk_size=5000, sample_size=1000):
    """
    åˆ†å—+é‡‡æ ·å»é‡ï¼Œé¿å…O(nÂ²)å¤æ‚åº¦é—®é¢˜
    
    Args:
        dataset: æ•°æ®é›†
        text_field: æ–‡æœ¬å­—æ®µ
        threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
        ngram_size: n-gramå¤§å°
        chunk_size: åˆ†å—å¤§å°
        sample_size: é‡‡æ ·å¤§å°
    """
    print(f"å¼€å§‹åˆ†å—å»é‡: æ€»æ•°æ®{len(dataset)}æ¡, åˆ†å—å¤§å°{chunk_size}, é‡‡æ ·å¤§å°{sample_size}")
    
    all_kept_indices = []
    processed_count = 0
    
    # åˆ†å—å¤„ç†
    for chunk_start in tqdm(range(0, len(dataset), chunk_size), desc="åˆ†å—å¤„ç†"):
        chunk_end = min(chunk_start + chunk_size, len(dataset))
        chunk = dataset.select(range(chunk_start, chunk_end))
        
        print(f"å¤„ç†åˆ†å— {chunk_start}-{chunk_end} ({len(chunk)} æ¡)")
        
        # å¯¹æ¯ä¸ªåˆ†å—ä½¿ç”¨å¿«é€Ÿå»é‡ï¼ˆé‡‡æ ·ä¼˜åŒ–ï¼‰
        deduplicated_chunk = quick_jaccard_deduplicate(
            chunk, text_field, threshold, ngram_size, sample_size
        )
        
        # è®°å½•ä¿ç•™çš„å…¨å±€ç´¢å¼•
        if len(deduplicated_chunk) > 0:
            # è·å–åˆ†å—å†…çš„ä¿ç•™ç´¢å¼•ï¼Œç„¶åè½¬æ¢ä¸ºå…¨å±€ç´¢å¼•
            chunk_kept_indices = list(range(len(deduplicated_chunk)))
            global_indices = [chunk_start + i for i in chunk_kept_indices]
            all_kept_indices.extend(global_indices)
        
        processed_count += len(chunk)
        print(f"åˆ†å—å®Œæˆ: ä¿ç•™ {len(deduplicated_chunk)}/{len(chunk)} æ¡")
    
    # æ„å»ºæœ€ç»ˆç»“æœ
    if all_kept_indices:
        final_dataset = dataset.select(all_kept_indices)
        print(f"åˆ†å—å»é‡å®Œæˆ: {len(dataset)} -> {len(final_dataset)} æ¡")
        return final_dataset
    else:
        print("è­¦å‘Š: æ‰€æœ‰æ•°æ®éƒ½è¢«å»é‡äº†")
        return dataset.select([])  # è¿”å›ç©ºæ•°æ®é›†

# åŠ è½½æ•°æ®é›†
dataset = load_dataset("ag_news")

# å®šä¹‰é˜ˆå€¼
thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]

# åˆ›å»ºè¾“å‡ºç›®å½•
os.makedirs("batch_cleaned_datasets", exist_ok=True)

for threshold in thresholds:
    print(f"\n{'='*60}")
    print(f"å¤„ç†é˜ˆå€¼: {threshold}")
    print(f"{'='*60}")
    
    # æ¸…ç†å…¨å±€å˜é‡ï¼ˆå¼€å§‹å‰å…ˆæ¸…ç†ï¼‰
    clear_global_memory()
    
    start_time = time.time()
    
    # åˆ†åˆ«å¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†è¿›è¡Œåˆ†å—å»é‡
    print("å¤„ç†è®­ç»ƒé›†...")
    cleaned_train = chunked_deduplication(
        dataset['train'], 
        'text', 
        threshold, 
        ngram_size=3, 
        chunk_size=5000,   # 5Kä¸€å—
        sample_size=1000   # æ¯å—å†…æœ€å¤šæ¯”è¾ƒ1000ä¸ª
    )
    
    print("\nå¤„ç†æµ‹è¯•é›†...")
    cleaned_test = chunked_deduplication(
        dataset['test'], 
        'text', 
        threshold, 
        ngram_size=3, 
        chunk_size=2000,   # æµ‹è¯•é›†è¾ƒå°ï¼Œ2Kä¸€å—
        sample_size=500    # æ¯å—å†…æœ€å¤šæ¯”è¾ƒ500ä¸ª
    )
    
    processing_time = time.time() - start_time
    
    # ä¿å­˜ç»“æœ
    train_path = f"batch_cleaned_datasets/ag_news_train_threshold_{threshold}.json"
    test_path = f"batch_cleaned_datasets/ag_news_test_threshold_{threshold}.json"
    
    cleaned_train.to_json(train_path)
    cleaned_test.to_json(test_path)
    
    # ç»Ÿè®¡ä¿¡æ¯
    train_reduction = (len(dataset['train']) - len(cleaned_train)) / len(dataset['train']) * 100
    test_reduction = (len(dataset['test']) - len(cleaned_test)) / len(dataset['test']) * 100
    total_processed = len(dataset['train']) + len(dataset['test'])
    speed = total_processed / processing_time if processing_time > 0 else 0
    
    print(f"\né˜ˆå€¼ {threshold} å¤„ç†å®Œæˆ!")
    print(f"è®­ç»ƒé›†: {len(dataset['train'])} -> {len(cleaned_train)} æ¡ (å‡å°‘ {train_reduction:.1f}%)")
    print(f"æµ‹è¯•é›†: {len(dataset['test'])} -> {len(cleaned_test)} æ¡ (å‡å°‘ {test_reduction:.1f}%)")
    print(f"å¤„ç†æ—¶é—´: {processing_time:.2f} ç§’")
    print(f"å¤„ç†é€Ÿåº¦: {speed:.0f} æ¡/ç§’")
    
    # æ¸…ç†å…¨å±€å˜é‡ï¼ˆç»“æŸåå†æ¸…ç†ï¼‰
    clear_global_memory()

print("\nğŸ‰ æ‰€æœ‰é˜ˆå€¼å¤„ç†å®Œæˆï¼")
